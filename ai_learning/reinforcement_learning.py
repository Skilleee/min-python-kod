import numpy as np
import pandas as pd
import logging
import gym
from stable_baselines3 import PPO
from gym import spaces

# Konfigurera loggning
logging.basicConfig(filename="reinforcement_learning.log", level=logging.INFO)

class TradingEnv(gym.Env):
    """
    Anpassad f√∂rst√§rkningsinl√§rningsmilj√∂ f√∂r trading.
    """
    def __init__(self, data):
        super(TradingEnv, self).__init__()
        self.data = data
        self.current_step = 0
        
        # Definiera action space: 0 = HOLD, 1 = BUY, 2 = SELL
        self.action_space = spaces.Discrete(3)
        
        # Definiera observationsutrymmet: Pris, momentum, volym
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32
        )
        
    def reset(self):
        """√Öterst√§ller milj√∂n till startl√§ge."""
        self.current_step = 0
        return self._next_observation()
    
    def _next_observation(self):
        """Returnerar n√§sta observation."""
        obs = self.data.iloc[self.current_step][["close", "momentum", "volume"]].values
        return obs.astype(np.float32)
    
    def step(self, action):
        """Utf√∂r en handling och returnerar observation, bel√∂ning och status."""
        self.current_step += 1
        
        done = self.current_step >= len(self.data) - 1
        reward = self._calculate_reward(action)
        
        return self._next_observation(), reward, done, {}
    
    def _calculate_reward(self, action):
        """Ber√§knar bel√∂ning baserat p√• om AI:n fattade r√§tt beslut."""
        if action == 1:  # BUY
            return self.data.iloc[self.current_step]["return"]
        elif action == 2:  # SELL
            return -self.data.iloc[self.current_step]["return"]
        return 0  # HOLD

# Tr√§na RL-modellen
if __name__ == "__main__":
    # Simulerad prisdata
    np.random.seed(42)
    data = pd.DataFrame({
        "close": np.cumsum(np.random.randn(1000) * 2 + 100),
        "momentum": np.random.randn(1000),
        "volume": np.random.randint(100, 1000, size=1000),
        "return": np.random.randn(1000) / 100
    })
    
    env = TradingEnv(data)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=10000)
    
    print("üì¢ Reinforcement Learning-modellen √§r tr√§nad och redo att testas!")
